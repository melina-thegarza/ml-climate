{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical Flood Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# historical flood data\n",
    "def clean_flood_data():\n",
    "    flood_df = pd.read_excel('FloodArchive.xlsx', engine='openpyxl')\n",
    "\n",
    "    #filter only for United States\n",
    "    flood_df = flood_df[flood_df['Country'] == 'USA']\n",
    "    #add a zip code based on the long and lat\n",
    "    \n",
    "    return flood_df\n",
    "\n",
    "flood_df = clean_flood_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA number of flood incidents: 477\n",
      "    ID GlideNumber Country OtherCountry      long      lat       Area  \\\n",
      "8    9           0     USA            0  -85.1742  40.6691  210527.96   \n",
      "10  11           0     USA            0  -89.5537  40.6814   26266.14   \n",
      "11  12           0     USA            0 -108.0930  35.3824   26527.13   \n",
      "12  13           0     USA            0  -96.7845  29.6044  141508.00   \n",
      "13  14           0     USA            0  -83.5377  42.0122   16883.54   \n",
      "\n",
      "        Began      Ended Validation  Dead  Displaced          MainCause  \\\n",
      "8  1985-02-22 1985-03-01       News     7       2250  Rain and snowmelt   \n",
      "10 1985-03-03 1985-03-08       News     4       2400  Rain and snowmelt   \n",
      "11 1985-03-13 1985-03-14       News     0         80  Rain and snowmelt   \n",
      "12 1985-03-14 1985-03-15       News     0          0         Heavy rain   \n",
      "13 1985-03-30 1985-03-31       News     0        300         Heavy rain   \n",
      "\n",
      "    Severity  \n",
      "8        2.0  \n",
      "10       2.0  \n",
      "11       1.0  \n",
      "12       1.0  \n",
      "13       1.0  \n"
     ]
    }
   ],
   "source": [
    "print(f'USA number of flood incidents: {len(flood_df)}')\n",
    "print(flood_df.head())\n",
    "#save as csv\n",
    "flood_df.to_csv('united_states_floods.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium\n",
    "import folium\n",
    "import math\n",
    "\n",
    "#visual representation of FLOOD DATA\n",
    "map_usa = folium.Map(location=[39.8283, -98.5795], zoom_start=5)\n",
    "severity_colors = {\n",
    "    1.0: \"yellow\", #large flood events: 1-2 decades-long reported interval since the last similar event\n",
    "    1.5: \"orange\", #very large events: greater than 2 decades but less than 100 year estimated recurrence interval\n",
    "    2.0: \"red\" #Extreme events: with an estimated recurrence interval greater than 100 years.\n",
    "}\n",
    "\n",
    "# Add a marker for each flood occurrence\n",
    "for index, row in flood_df.iterrows():\n",
    "    # Add a circle marker at each flood location (lat, long)\n",
    "    folium.CircleMarker(\n",
    "        location=[row['lat'], row['long']],\n",
    "        popup=f\"ID: {row['ID']} | Severity: {row['Severity']} | Displaced: {row['Displaced']} | Date: {row['Began']}\",\n",
    "        color=severity_colors[row['Severity']] ,\n",
    "        fill=True,\n",
    "        fill_opacity=0.6\n",
    "    ).add_to(map_usa)\n",
    "\n",
    "# Save map to HTML file\n",
    "map_usa.save('flood_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install uszipcode\n",
    "\n",
    "#add zipcode column to flood data\n",
    "from uszipcode import SearchEngine\n",
    "\n",
    "search = SearchEngine()\n",
    "\n",
    "def get_zipcode(lat, lon):\n",
    "    result = search.by_coordinates(lat, lon)\n",
    "    if result:\n",
    "        return result[0].zipcode\n",
    "    return None\n",
    "\n",
    "# Apply the get_zipcode function to each row and create a new column 'zipcode'\n",
    "flood_df['zipcode'] = flood_df.apply(lambda row: get_zipcode(row['lat'], row['long']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID GlideNumber Country OtherCountry      long      lat       Area  \\\n",
      "8    9           0     USA            0  -85.1742  40.6691  210527.96   \n",
      "10  11           0     USA            0  -89.5537  40.6814   26266.14   \n",
      "11  12           0     USA            0 -108.0930  35.3824   26527.13   \n",
      "12  13           0     USA            0  -96.7845  29.6044  141508.00   \n",
      "13  14           0     USA            0  -83.5377  42.0122   16883.54   \n",
      "\n",
      "        Began      Ended Validation  Dead  Displaced          MainCause  \\\n",
      "8  1985-02-22 1985-03-01       News     7       2250  Rain and snowmelt   \n",
      "10 1985-03-03 1985-03-08       News     4       2400  Rain and snowmelt   \n",
      "11 1985-03-13 1985-03-14       News     0         80  Rain and snowmelt   \n",
      "12 1985-03-14 1985-03-15       News     0          0         Heavy rain   \n",
      "13 1985-03-30 1985-03-31       News     0        300         Heavy rain   \n",
      "\n",
      "    Severity zipcode  \n",
      "8        2.0   46781  \n",
      "10       2.0   61611  \n",
      "11       1.0   87045  \n",
      "12       1.0   78962  \n",
      "13       1.0   48159  \n"
     ]
    }
   ],
   "source": [
    "#save as csv\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(flood_df.head())\n",
    "flood_df.to_csv('united_states_floods.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Historical Housing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_zipcodes = flood_df[~flood_df['zipcode'].isna()]['zipcode'].unique()\n",
    "def clean_housing_data():\n",
    "    df = pd.read_csv('hpi_at_bdl_zip5.csv', dtype={'Five-Digit ZIP Code': str})\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    #zipcodes from flood data\n",
    "    df = df[df['Five-Digit ZIP Code'].isin(valid_zipcodes)]\n",
    "    return df\n",
    "\n",
    "df_housing = clean_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Five-Digit ZIP Code  Year  Annual Change (%)     HPI  HPI with 1990 base  \\\n",
      "2401               01253  2001                NaN  100.00                 NaN   \n",
      "2402               01253  2002               9.38  109.38                 NaN   \n",
      "2403               01253  2003               2.17  111.75                 NaN   \n",
      "2404               01253  2004              21.80  136.12                 NaN   \n",
      "2405               01253  2005               8.09  147.13                 NaN   \n",
      "2406               01253  2006              12.92  166.14                 NaN   \n",
      "2407               01253  2007              -2.73  161.60                 NaN   \n",
      "2408               01253  2008              -1.61  159.00                 NaN   \n",
      "2409               01253  2009              -2.66  154.77                 NaN   \n",
      "2410               01253  2010               0.33  155.28                 NaN   \n",
      "\n",
      "      HPI with 2000 base  \n",
      "2401                 NaN  \n",
      "2402                 NaN  \n",
      "2403                 NaN  \n",
      "2404                 NaN  \n",
      "2405                 NaN  \n",
      "2406                 NaN  \n",
      "2407                 NaN  \n",
      "2408                 NaN  \n",
      "2409                 NaN  \n",
      "2410                 NaN  \n"
     ]
    }
   ],
   "source": [
    "print(df_housing.head(10))\n",
    "#save as csv\n",
    "df_housing.to_csv('united_states_housing.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for predicing HPI based on flood data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
